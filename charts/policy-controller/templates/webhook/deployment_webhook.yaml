apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    {{- include "policy-controller.labels" . | nindent 4 }}
    control-plane: {{ template "policy-controller.fullname" . }}-webhook
  name: {{ template "policy-controller.fullname" . }}-webhook
  namespace: {{ .Release.Namespace }}
spec:
  replicas: {{ .Values.webhook.replicaCount }}
  selector:
    matchLabels:
      {{- include "policy-controller.selectorLabels" . | nindent 6 }}
      control-plane: {{ template "policy-controller.fullname" . }}-webhook
  template:
    metadata:
      labels:
        {{- include "policy-controller.selectorLabels" . | nindent 8 }}
        control-plane: {{ template "policy-controller.fullname" . }}-webhook
    spec:
      nodeSelector:
        {{- toYaml .Values.commonNodeSelector | nindent 8 }}
      tolerations:
        {{- toYaml .Values.commonTolerations | nindent 8 }}
      serviceAccountName: {{ template "policy-controller.fullname" . }}-webhook
      # To avoid node becoming SPOF, spread our replicas to different nodes.
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  control-plane: {{ template "policy-controller.fullname" . }}-webhook
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
      - name: {{ template "policy-controller.name" . }}-{{ .Values.webhook.name }}
        image: "{{ template "policy-controller.image" .Values.webhook.image }}"
        imagePullPolicy: "{{ .Values.webhook.image.pullPolicy }}"
        env:
        - name: SYSTEM_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: CONFIG_LOGGING_NAME
          value: {{ template "policy-controller.fullname" . }}-webhook-logging
        - name: CONFIG_OBSERVABILITY_NAME
          value: {{ template "policy-controller.fullname" . }}-webhook-observability
        - name: METRICS_DOMAIN
          value: sigstore.dev/policy
        - name: WEBHOOK_NAME
          value: webhook
{{- if .Values.webhook.env }}
{{- range $key, $value := .Values.webhook.env }}
        - name: "{{ $key }}"
          value: "{{ $value }}"
{{- end }}
{{- end }}
        args:
        {{- if semverCompare ">= 1.8-0" .Chart.AppVersion }}
        - -webhook-name={{ required "A valid cosign.webhookName is required" .Values.cosign.webhookName }}
        {{- end }}
        {{- if and .Values.cosign.secretKeyRef }}
        {{- if .Values.cosign.secretKeyRef.name }}
        - -secret-name={{ .Values.cosign.secretKeyRef.name }}
        {{- end }}
        {{- else }}
        - -secret-name={{ template "policy-controller.fullname" . }}-cosign-key
        {{- end }}
        {{- range $key, $value := .Values.webhook.extraArgs }}
        - -{{ $key }}={{ $value }}
        {{- end }}
        ports:
        - containerPort: 8443
          name: https
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        resources:
        {{- with .Values.webhook.resources }}
          {{- toYaml . | nindent 10 }}
        {{- end }}
        livenessProbe:
          failureThreshold: 6
          initialDelaySeconds: 30
          periodSeconds: 1
          httpGet:
            port: 8443
            scheme: HTTPS
            path: /healthz
            httpHeaders:
            - name: k-kubelet-probe
              value: "webhook"
        readinessProbe:
          failureThreshold: 6
          initialDelaySeconds: 20
          periodSeconds: 1
          httpGet:
            port: 8443
            scheme: HTTPS
            path: /readyz
            httpHeaders:
            - name: k-kubelet-probe
              value: "webhook"
{{- if .Values.webhook.podSecurityContext.enabled }}
        securityContext:
        {{- with .Values.webhook.podSecurityContext }}
        {{- omit . "enabled" | toYaml | nindent 10}}
        {{- end }}
{{- end }}
      {{- if .Values.webhook.securityContext.enabled }}

      # Our webhook should gracefully terminate by lame ducking first, set this to a sufficiently
      # high value that we respect whatever value it has configured for the lame duck grace period.
      terminationGracePeriodSeconds: 300

      securityContext:
        {{- with .Values.webhook.securityContext }}
        {{- toYaml . | nindent 8}}
        {{- end }}
      {{- end }}
